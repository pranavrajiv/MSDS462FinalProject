{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your pre-trained keras model to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.6\n",
      "  Downloading tensorflow-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |████████████████████████████████| 458.3 MB 9.1 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.37.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (1.42.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (0.36.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (0.2.0)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 57.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (1.1.0)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 48.7 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (1.19.3)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 63.6 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 60.2 MB/s            \n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 74.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow==2.6) (3.15.2)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow==2.6) (1.5.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 64.1 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 40.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow==2.6) (2.26.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6) (4.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6) (1.26.8)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.6) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6) (3.1.1)\n",
      "Building wheels for collected packages: clang\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=a4d06ec235f81705d35f58eeb0db994dbe5357689c2dd267d994fe5d64e98b29\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "Successfully built clang\n",
      "Installing collected packages: typing-extensions, tensorboard-plugin-wit, tensorboard-data-server, absl-py, tensorflow-estimator, tensorboard, keras-preprocessing, keras, h5py, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.0.0\n",
      "    Uninstalling absl-py-1.0.0:\n",
      "      Successfully uninstalled absl-py-1.0.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.1\n",
      "    Uninstalling tensorboard-2.1.1:\n",
      "      Successfully uninstalled tensorboard-2.1.1\n",
      "  Attempting uninstall: keras-preprocessing\n",
      "    Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.3.0\n",
      "    Uninstalling Keras-2.3.0:\n",
      "      Successfully uninstalled Keras-2.3.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-serving-api 2.1.0 requires tensorflow~=2.1.0, but you have tensorflow 2.6.0 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires gast==0.2.2, but you have gast 0.4.0 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires h5py<=2.10.0, but you have h5py 3.1.0 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires keras-preprocessing==1.1.0, but you have keras-preprocessing 1.1.2 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.3 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires tensorboard<2.2.0,>=2.1.0, but you have tensorboard 2.9.0 which is incompatible.\n",
      "tensorflow-cpu 2.1.3 requires tensorflow-estimator<2.2.0,>=2.1.0rc0, but you have tensorflow-estimator 2.8.0 which is incompatible.\n",
      "pylint 2.12.2 requires typing-extensions>=3.10.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "astroid 2.9.0 requires typing-extensions>=3.10; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.24.42 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 clang-5.0 flatbuffers-1.12 gast-0.4.0 h5py-3.1.0 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.0 tensorflow-estimator-2.8.0 typing-extensions-3.7.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.8.3)\n",
      "Requirement already satisfied: numpy==1.19.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (1.19.3)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (4.5.1.48)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (0.15.0)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (0.8)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (3.15.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from mediapipe) (0.36.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# if your model is saved as only a .h5 file\n",
    "MODEL_LOCATION ='sampleModel'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Model\n",
    "Simply run the cell below; the model will be loaded based on how you defined the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from MODEL_LOCATION\n"
     ]
    }
   ],
   "source": [
    "if MODEL_LOCATION!='': #if your model is saved as a .h5 file only\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import load_model\n",
    "    model = load_model(MODEL_LOCATION, compile = False)#ad the model\n",
    "    print(\"loaded model from MODEL_LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert the Keras Model to the format AWS wants\n",
    "- Converts to a Protobuff file\n",
    "- Saves it in a certain aws file structure\n",
    "- Tarballs this file and zips it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export/Servo/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "def convert_to_aws(loaded_model):\n",
    "    \"\"\"\n",
    "    given a pre-trained keras model, this function converts it to a TF protobuf format\n",
    "    and saves it in the file structure which aws expects\n",
    "    \"\"\"  \n",
    "    from tensorflow.python.saved_model import builder\n",
    "    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "    from tensorflow.python.saved_model import tag_constants\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import shutil\n",
    "    if tf.executing_eagerly():\n",
    "       tf.compat.v1.disable_eager_execution()\n",
    "    dirpath = 'export'\n",
    "    if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "        shutil.rmtree(dirpath)\n",
    "    \n",
    "    # This is the file structure which AWS expects. Cannot be changed. \n",
    "    model_version = '1'\n",
    "    export_dir = 'export/Servo/' + model_version\n",
    "    \n",
    "    # Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "    builder = builder.SavedModelBuilder(export_dir)\n",
    "    \n",
    "    # Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "    signature = predict_signature_def(\n",
    "        inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})\n",
    "    \n",
    "    session = tf.compat.v1.Session()\n",
    "    init_op = tf.compat.v1.global_variables_initializer()\n",
    "    session.run(init_op)\n",
    "    # Save the meta graph and variables\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=session, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    builder.save() \n",
    "    \n",
    "    #create a tarball/tar file and zip it\n",
    "    import tarfile\n",
    "    with tarfile.open('model_deploy.tar.gz', mode='w:gz') as archive:\n",
    "        archive.add('export', recursive=True)\n",
    "        \n",
    "convert_to_aws(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Move the tarball (tar.gz) to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is: sagemaker-us-east-1-852069333125\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model_deploy.tar.gz', key_prefix='model')\n",
    "\n",
    "# View details of the uploaded bucket\n",
    "print(f\"Bucket name is: {sagemaker_session.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the name of the bucket which SageMaker made in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-852069333125'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where did it upload to?\n",
    "print(\"Bucket name is:\")\n",
    "sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a SageMaker Model\n",
    "First, create an empty train.py file (TensorFlowModel expects this at its 'entry point', but can be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch train.py #create an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# the (default) IAM role you created when creating this notebook\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create a Sagemaker model (see AWS console>SageMaker>Models)\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model_deploy.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = tf.__version__,\n",
    "                                  entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a) Host the SageMaker model and\n",
    "## 6b) Create an Endpoint to access the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the message `update_endpoint is a no-op in sagemaker>=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "deployement_instance_type = \"ml.m4.xlarge\"\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=deployement_instance_type)\n",
    "endpoint = predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-inference-2022-05-21-06-57-20-604'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success! You have deployed a keras model into AWS\n",
    "# ---------------------\n",
    "### 7. Confirm its working correctly by making a prediction\n",
    "Now, we want to use our endpoint/model. Create a predictor which uses the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "    \n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        \n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    \n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    \n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    \n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in labels_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "labels_dict = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25,'space':26,'del':27,'nothing':28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0344865844,\n",
       "  0.0345011503,\n",
       "  0.0345036052,\n",
       "  0.0344779491,\n",
       "  0.0344655961,\n",
       "  0.0344716683,\n",
       "  0.0344790183,\n",
       "  0.0344779193,\n",
       "  0.0344862677,\n",
       "  0.0344822742,\n",
       "  0.0344918594,\n",
       "  0.0344807506,\n",
       "  0.0344863646,\n",
       "  0.0344770104,\n",
       "  0.0344935954,\n",
       "  0.0344843082,\n",
       "  0.0344746634,\n",
       "  0.0344893746,\n",
       "  0.0344714597,\n",
       "  0.0344845653,\n",
       "  0.0344930626,\n",
       "  0.0344822407,\n",
       "  0.0344895683,\n",
       "  0.0344705805,\n",
       "  0.0344851501,\n",
       "  0.0344789103,\n",
       "  0.0344848745,\n",
       "  0.0344713852,\n",
       "  0.0344783515]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonData[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label\n",
      "C\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import json\n",
    "\n",
    "\n",
    "endpoint = 'tensorflow-inference-2022-05-21-06-57-20-604' #get endpoint name from SageMaker > endpoints\n",
    "\n",
    "predictor=sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=CSVSerializer())\n",
    "\n",
    "#success, image = cap.read()\n",
    "image = cv2.imread(\"W_test.jpg\", cv2.IMREAD_COLOR)\n",
    "imgRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(imgRGB)\n",
    "predictions=\"\"\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks, handedness in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "        landmark_list = calc_landmark_list(imgRGB, hand_landmarks)\n",
    "        pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "        data = np.array([pre_processed_landmark_list], dtype=np.float32)\n",
    "        predictions = predictor.predict(data)\n",
    "        my_json = predictions.decode(\"utf-8\").replace(\"'\", '\"')\n",
    "        jsonData = json.loads(my_json)\n",
    "        classes_x=np.argmax(np.squeeze(jsonData[\"predictions\"]))\n",
    "        print(\"\\nLabel\")\n",
    "        print(get_key(classes_x))\n",
    "        print(\"\\n\")\n",
    "        cv2.putText(image, get_key(classes_x),(10,60), cv2.FONT_HERSHEY_PLAIN,3, (255,0,255),4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access SageMaker endpoint using lamda function and Api gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0344889313, 0.0345029645, 0.0345036276, 0.0344779342, 0.0344665162, 0.0344712362, 0.0344801955, 0.034477856, 0.0344871096, 0.0344826579, 0.0344917662, 0.034480419, 0.0344854817, 0.0344761573, 0.0344930738, 0.0344839618, 0.0344761759, 0.0344885178, 0.0344721, 0.0344843306, 0.0344948731, 0.0344801247, 0.034489, 0.0344701111, 0.0344847441, 0.0344773047, 0.034485057, 0.0344713032, 0.0344764814]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "URL = 'https://pgjw2grj50.execute-api.us-east-1.amazonaws.com/test/predict'\n",
    "header = {\"Content-Type\":\"application/json\"}\n",
    "data_payload = '{\"data\":\"0.0,0.0,0.14117648,-0.01764706,0.2,-0.1882353,0.05882353,-0.30588236,-0.0882353,-0.3764706,0.27058825,-0.44117647,0.3647059,-0.63529414,0.41764706,-0.7764706,0.44117647,-0.9,0.14117648,-0.4882353,0.1882353,-0.7176471,0.20588236,-0.87058824,0.21176471,-1.0,0.02352941,-0.46470588,-0.01764706,-0.67058825,-0.04117647,-0.81764704,-0.05882353,-0.9588235,-0.0882353,-0.3882353,-0.0882353,-0.42941177,-0.07058824,-0.32941177,-0.06470589,-0.24705882\"}'\n",
    "request = requests.request(\"POST\",URL, headers=header, data=data_payload)\n",
    "print(request.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup!\n",
    "\n",
    "else you will incur extra charges\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html\n",
    "\n",
    "- Stop Notebook\n",
    "- delete endpoints\n",
    "- delete models\n",
    "- delete S3 bucket\n",
    "- delete cloudwatch groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
